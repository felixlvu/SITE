import cv2
import numpy as np
from retinaface import RetinaFace
from facenet_pytorch import InceptionResnetV1
import os
import json
import pickle

def load_recognition_model(database_path, save_file):
    if os.path.exists(save_file):
        print(f"Chargement des données d'entraînement à partir de {save_file}...")
        with open(save_file, 'rb') as f:
            saved_data = pickle.load(f)
        return saved_data

    print("Modèle de reconnaissance faciale: InceptionResnetV1")

    resnet = InceptionResnetV1(pretrained='vggface2').eval()

    database_embeddings = {}
    database_names = []

    print("Entraînement du modèle de reconnaissance faciale...")

    for filename in os.listdir(database_path):
        if filename.endswith('.mp4'):
            person_name = os.path.splitext(filename)[0]
            person_video_path = os.path.join(database_path, filename)
            person_cap = cv2.VideoCapture(person_video_path)

            embeddings = []
            frame_count = int(person_cap.get(cv2.CAP_PROP_FRAME_COUNT))

            for _ in range(frame_count):
                ret, frame = person_cap.read()
                if not ret:
                    break

                faces = RetinaFace.detect_faces(frame)
                if isinstance(faces, dict):
                    for face_id, face_dict in faces.items():
                        bbox_list = face_dict['facial_area']
                        x, y, x1, y1 = map(int, bbox_list)

                        face = frame[y:y1, x:x1]

                        new_size = (160, 160)
                        resized_face = cv2.resize(face, new_size)

                        face_tensor = (
                            torch.from_numpy(resized_face)
                            .permute(2, 0, 1)
                            .float()
                            / 255.0
                        )

                        embedding = resnet(face_tensor.unsqueeze(0)).squeeze().detach().numpy()
                        embeddings.append(embedding)

            if len(embeddings) > 0:
                database_embeddings[person_name] = np.mean(embeddings, axis=0)
                database_names.append(person_name)

    data_to_save = (resnet, database_embeddings, database_names)
    with open(save_file, 'wb') as f:
        pickle.dump(data_to_save, f)

    return resnet, database_embeddings, database_names

def predict_person(embedding, database_embeddings, database_names, threshold=0.64):
    best_match = " "
    best_similarity = threshold 

    for name, db_embedding in database_embeddings.items():
        similarity = np.dot(embedding, db_embedding) / (np.linalg.norm(embedding) * np.linalg.norm(db_embedding))
        if similarity > best_similarity:
            best_similarity = similarity
            best_match = name

    return best_match

def process_video_and_generate_json(video_path, resnet, database_embeddings, database_names, output_json):
    cap = cv2.VideoCapture(video_path)

    # Liste pour stocker les informations sur les personnes détectées
    detected_people = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        faces = RetinaFace.detect_faces(frame)
        if isinstance(faces, dict):
            for face_id, face_dict in faces.items():
                bbox_list = face_dict['facial_area']
                x, y, x1, y1 = map(int, bbox_list)

                face = frame[y:y1, x:x1]

                new_size = (160, 160)
                resized_face = cv2.resize(face, new_size)

                face_tensor = (
                    torch.from_numpy(resized_face)
                    .permute(2, 0, 1)
                    .float()
                    / 255.0
                )

                embedding = resnet(face_tensor.unsqueeze(0)).squeeze().detach().numpy()

                predicted_name = predict_person(embedding, database_embeddings, database_names)

                # Ajoutez les informations sur la personne détectée à la liste
                detected_people.append(predicted_name)

    # Enregistrez les informations dans un fichier JSON
    with open(output_json, 'w') as json_file:
        json.dump({"DetectedPeople": detected_people}, json_file, indent=4)

if __name__ == "__main__":
    database_path = 'database'
    save_file = 'training_data.pkl'
    resnet, database_embeddings, database_names = load_recognition_model(database_path, save_file)

    video_path = 'input_video.mp4'  # Mettez ici le chemin vers votre vidéo d'entrée
    output_json = 'output.json'

    process_video_and_generate_json(video_path, resnet, database_embeddings, database_names, output_json)
